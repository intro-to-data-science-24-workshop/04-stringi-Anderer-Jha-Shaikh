---
Title: "stringi: Fast and Portable Character String Processing Facilities"
author: "Benedict Anderer,Farhan Shaikh , Saurav Jha"
date: "`r format(Sys.time(), '%B %d, %Y | %H:%M:%S | %Z')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: no
---

<style>
div.answer {background-color:#f3f0ff; border-radius: 5px; padding: 20px;} 
div.comment {background-color:#F0F6FF; border-radius: 5px; padding: 20px;}
</style> 

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)
``` 

Install the packages required for this task 

```{r, include = T}
library(rvest)
library(stringi)
library(httr)  
```

Case Study:- Using stringi for web scraping and data preparation. 

(a) Fetch Berlin's wikipedia page, extract text from each table and display the structure of extracted text  

(b) Harvest Berlin's climate data and transform it for statistical analysis using stringi, compute monthly range of temperatures. 
{Hint}:- Convert the dataframe into matrix to vectorise the processing of every single cell. Advantages:- All elements are of same type- string manipulation easy to apply 


```{r} 

# Define the URL
url<- "https://en.wikipedia.org/wiki/Berlin"  

# Read the HTML content from the page
page <- read_html(url)   

# Extract all tables
all_tables <- page %>%
  html_nodes("table") 

# Extract text from each table
text_tables <- sapply(all_tables, html_text)

# Display the structure of the extracted text
str(text_tables, nchar.max = 65, vec.len = 5, strict.width = "wrap")

# html_text() extracts only the textual content from the tables, removing all HTML tags and formatting. 

# This results in a plain text representation, making it easier to read but lacking the original table structure. 


```

```{r}   

# Using stringi's string_detect function 

# Following line searches the text_tables vector for the presence of the phrase "climate data" (case-insensitive), returns the index of first occurence and assigns it to idx

idx <- which(stri_detect_fixed(text_tables, "climate data",
case_insensitive=TRUE, max_count=1))
 
x <- as.data.frame(html_table(all_tables[[idx]], fill=TRUE))

head(x) 

``` 
```{r}   
x <- as.matrix(x) # Convert the data into matrix so that cells can be vectorised  
x[, ] <- stri_trans_general(x, "Publishing-Any; Any-ASCII")  # converts (“−”) to ASCII equivalents using transliterator 

dimnames(x) <- list(x[, 1], x[1, ]) # Sets the row names to the values in the first column and the column names to the values in the first row. 

x <- x[2:(nrow(x) - 1), 2:ncol(x)] # skip 1st/last row and 1st column

x[, c(1, ncol(x))] # Displays the first and last columns 

x <- x[-nrow(x), ]  # remove the last row 

``` 


```{r} 

# Step 1: Remove commas used as thousand separators
x[,] <- stri_replace_all_regex(x, ",", "")

# Step 2: Extract numeric values, removing anything in parentheses including "(°F)" and "(inches)"
x[,] <- stri_replace_all_regex(x, "\\s*\\(.*?\\)|\\s*°F|\\s*inches", "")

# Step 3: Remove "(°F)" and "(inches)" from the dimension names if they exist
dimnames(x)[[1]] <- stri_replace_all_regex(dimnames(x)[[1]], "\\s*\\(°F\\)|\\s*\\(inches\\)", "")

# Step 4: Convert the cleaned strings to numeric
x <- apply(x, c(1, 2), function(val) as.numeric(val))

# Step 5: Remove the last row
x <- x[-nrow(x), ] 

# Display the cleaned matrix
print(x)

```

```{r}  

# Now we can start computation, for example monthly temperature ranges    

x["Record high °C", -ncol(x)] - x["Record low °C", -ncol(x)] 

```

